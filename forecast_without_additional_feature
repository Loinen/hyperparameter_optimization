import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import timeit
import seaborn as sns
import datetime
import random

from sklearn.metrics import mean_squared_error

from fedot.core.data.data import InputData
from fedot.core.pipelines.node import PrimaryNode, SecondaryNode
from fedot.core.pipelines.pipeline import Pipeline
from fedot.core.pipelines.tuning.unified import PipelineTuner
from fedot.core.repository.dataset_types import DataTypesEnum
from fedot.core.repository.tasks import Task, TaskTypesEnum, TsForecastingParams

from main import plot_series, interpolate, comparsion_plot, count_errors


def loss_plot(data_array, title):
    now_dt = datetime.datetime.now()
    now = now_dt.strftime("%d_%m_%Y_%H-%M-%S")
    print("loss: ", data_array)
    plt.plot(data_array)
    plt.title('Loss history')
    plt.ylabel('mse')
    plt.xlabel('iteration')
    plt.savefig('results/{title}_{now}'.format(title=title, now=now))
    plt.show()


def prepare_input_data(len_forecast, train_data_features, train_data_target, test_data_features):
    task = Task(TaskTypesEnum.ts_forecasting, TsForecastingParams(len_forecast))

    train_input = InputData(idx=np.arange(0, len(train_data_features)),
                            features=train_data_features, target=train_data_target,
                            task=task, data_type=DataTypesEnum.ts)

    start_forecast = len(train_data_features)
    end_forecast = start_forecast + len_forecast
    predict_input = InputData(idx=np.arange(0, end_forecast),
                              features=np.concatenate([train_data_features, test_data_features]),
                              target=None, task=task, data_type=DataTypesEnum.ts)

    return train_input, predict_input, task


def run_experiment_with_tuning(time_series, col_name, len_forecast=250, cv_folds=None):
    # Let's divide our data on train and test samples
    train_data = time_series[:-len_forecast]
    test_data = time_series[-len_forecast:]

    # Source time series
    train_input, predict_input, task = prepare_input_data(len_forecast, train_data_features=train_data,
                                                          train_data_target=train_data,
                                                          test_data_features=train_data)

    init_pipeline = fourth_level_pipe()
    init_pipeline.show()
    print("init pipe")
    init_pipeline.print_structure()

    old_predicted, new_predicted, tune_time = \
        make_forecast_with_tuning(init_pipeline, train_input, predict_input, task, cv_folds)

    old_predicted = np.ravel(np.array(old_predicted))
    new_predicted = np.ravel(np.array(new_predicted))
    test_data = np.ravel(test_data)

    # plot losses
    outfile = "C:/Users/User/PycharmProjects/hyperparameter_optimization/results/loss_hyperopt/losses.npy"
    test = np.load(outfile)
    loss_plot(test, "pipeline_tuner")

    mse_init, mae_init, mse_tuning, mae_tuning = count_errors(test_data, old_predicted, new_predicted)

    start_point = len(time_series) - len_forecast * 2
    comparsion_plot(col_name, ts, old_predicted, new_predicted, len(train_data), 0,
                    "Init pipeline vs tuned (green) pipeline")
    comparsion_plot(col_name, ts, old_predicted, new_predicted, len(train_data), start_point,
                    "Init pipeline vs tuned (green) pipeline")
    # random search
    start_time = timeit.default_timer()
    results_rs, history = baseline(train_input)
    amount_of_seconds = timeit.default_timer() - start_time
    print(f'\nIt takes {amount_of_seconds:.2f} seconds to tune RS pipeline\n')
    loss_plot(history, "RS")

    rs_pipeline = fourth_level_pipe(results_rs)
    rs_pipeline.print_structure()

    rs_pipeline.fit_from_scratch(train_input)
    predicted_values = rs_pipeline.predict(predict_input)
    rs_predicted_values = predicted_values.predict
    rs_predicted_values = np.ravel(np.array(rs_predicted_values))

    mse_init, mae_init, mse_rs, mae_rs = count_errors(test_data, old_predicted, rs_predicted_values)

    start_point = len(time_series) - len_forecast * 2
    comparsion_plot(col_name, ts, old_predicted, rs_predicted_values, len(train_data), start_point,
                    "Init pipeline vs RS (green) pipeline")
    comparsion_plot(col_name, ts, new_predicted, rs_predicted_values, len(train_data), start_point,
                    "Tuned pipeline vs RS (green) pipeline")

    now_dt = datetime.datetime.now()
    now = now_dt.strftime("%d_%m_%Y_%H-%M-%S")
    errors_str = ' '.join(str(x) for x in [mse_init, mae_init, mse_tuning, mae_tuning])
    errors_rs = ' '.join(str(x) for x in [mse_rs, mae_rs])

    with open('results/errors/errors_{now}.txt'.format(now=now), 'w') as f:
        f.write('mse_init, mae_init, mse_tuning, mae_tuning  \n' + errors_str + '\ntune_time  \n' + str(tune_time) +
                'mse_rs, mae_rs   \n' + errors_rs + '\nrs_tune_time   \n' + str(amount_of_seconds))

    res = [mse_init, mae_init, mse_tuning, mae_tuning, mse_rs, mae_rs, tune_time, amount_of_seconds]
    res_df = pd.DataFrame([res], columns=['mse_init', 'mae_init', 'mse_tuning', 'mae_tuning', 'mse_rs', 'mae_rs',
                                          'tune_time', 'rs_time'])
    print(res_df)
    res_df.to_csv('results/errors/res_df_{now}.csv'.format(now=now), index=False)
    return res_df


def make_forecast_with_tuning(pipeline, train_input, predict_input, task, cv_folds):
    pipeline.fit_from_scratch(train_input)
    predicted_values = pipeline.predict(predict_input)
    old_predicted_values = predicted_values.predict

    pipeline_tuner = PipelineTuner(pipeline, task, iterations=10, timeout=datetime.timedelta(minutes=3))

    start_time = timeit.default_timer()
    pipeline = pipeline_tuner.tune_pipeline(input_data=train_input, loss_function=mean_squared_error,
                                            loss_params={'squared': False}, cv_folds=cv_folds, validation_blocks=3)
    amount_of_seconds = timeit.default_timer() - start_time
    print(f'\nIt takes {amount_of_seconds:.2f} seconds to tune pipeline\n')

    pipeline.fit_from_scratch(train_input)  # Fit pipeline on the entire train data
    print("PipelineTuner pipe")
    pipeline.print_structure()
    # Predict tuned
    predicted_values = pipeline.predict(predict_input)
    new_predicted_values = predicted_values.predict

    return old_predicted_values, new_predicted_values, amount_of_seconds


def fourth_level_pipe(start_params=None):
    """
    Pipeline looking like this
    smoothing - lagged - ridge \
                                \
                                 ridge -> final forecast
                                /
                lagged - ridge /
    """

    # First level
    node_smoothing = PrimaryNode('smoothing')
    if start_params is not None:
        print('start params:', start_params)
        node_smoothing.custom_params = {'window_size': start_params['window_size_smooth']}

    # Second level
    node_lagged_1 = SecondaryNode('lagged', nodes_from=[node_smoothing])
    node_lagged_1.custom_params = {'window_size': 300}  # 300 и 50 - результаты по прошлым запускам
    node_lagged_2 = PrimaryNode('lagged')
    node_lagged_2.custom_params = {'window_size': 50}

    if start_params is not None:
        node_lagged_1.custom_params = {'window_size': start_params['window_size']}
        node_lagged_2.custom_params = {'window_size': start_params['window_size2']}

    # Third level
    node_ridge_1 = SecondaryNode('ridge', nodes_from=[node_lagged_1])
    node_ridge_2 = SecondaryNode('ridge', nodes_from=[node_lagged_2])

    if start_params is not None:
        node_ridge_1.custom_params = {'alpha': start_params['alpha']}
        node_ridge_2.custom_params = {'alpha': start_params['alpha2']}

    # Fourth level - root node
    node_final = SecondaryNode('ridge', nodes_from=[node_ridge_1, node_ridge_2])
    if start_params is not None:
        node_final.custom_params = {'alpha': start_params['alpha3']}

    pipeline = Pipeline(node_final)
    return pipeline


def third_level_pipe():
    """
    smoothing - lagged   \
                          \
                            ridge -> final forecast
                          /
                lagged   /
    """

    # First level
    node_smoothing = PrimaryNode('smoothing')

    # Second level
    node_lagged_1 = SecondaryNode('lagged', nodes_from=[node_smoothing])
    node_lagged_1.custom_params = {'window_size': 300}  # 300 и 50 - результаты по прошлым запускам
    node_lagged_2 = PrimaryNode('lagged')
    node_lagged_2.custom_params = {'window_size': 50}

    # Third level - root node
    node_final = SecondaryNode('ridge', nodes_from=[node_lagged_1, node_lagged_2])
    pipeline = Pipeline(node_final)

    return pipeline


def baseline(train_data):
    min_mse = 9999
    num_iters = 10
    real_data = np.ravel(train_data.target)
    best_params = None
    mse_array = []

    for iteration in range(num_iters):
        alpha_range = np.arange(0.01, 10, 0.01)
        lagged = np.arange(5, 500, 1)
        smooth = np.arange(2, 20, 1)
        params = dict(alpha=random.choice(alpha_range), alpha2=random.choice(alpha_range),
                      window_size=random.choice(lagged), window_size_smooth=random.choice(smooth),
                      alpha3=random.choice(alpha_range), window_size2=random.choice(lagged))

        pipeline = fourth_level_pipe(params)
        pipeline.fit_from_scratch(train_data)
        predicted_values = pipeline.predict(train_data)
        predicted_values = predicted_values.predict
        predicted_values = np.ravel(np.array(predicted_values))
        mse = mean_squared_error(real_data[len(real_data) - 500:], predicted_values, squared=False)
        mse_array.append(mse)

        if mse < min_mse:
            min_mse = mse
            best_params = params

    print("best params is: ", best_params)
    print("min_mse is: ", min_mse)

    return best_params, mse_array


if __name__ == "__main__":
    data = pd.read_excel("kaggle/well_log.xlsx", sheet_name=0, nrows=3000)
    pd.set_option('display.max_columns', 50)
    # убираем лишние столбцы
    data.drop(columns=['SXO', 'Dtsyn', 'Vpsyn', 'sw new', 'sw new%', 'PHI2', 'ΔVp (m/s)', 'ΔVp'], inplace=True)
    print(data)

    plot_series(data)
    plot_series(data[:100])

    corr = data.corr()  # рисуем корреляционную матрицу
    sns.heatmap(corr, annot=True, fmt='.1f', cmap='Blues')
    plt.show()

    data = interpolate(data)
    # our columns - ['Depth', 'CALI', 'CGR', 'DT', 'ILD', 'NPHI', 'PEF', 'PHIE',
    # 'RHOB', 'RT', 'SGR', 'SW', 'Φda', 'ΦN', 'ΦND', 'Vpreal', 'ΔVp']
    # we need to rename some of them
    dict_columns = {'Φda': 'FDA',
                    'ΦN': 'FN',
                    'ΦND': 'FND',
                    'Vpreal': 'VP'}
    data.rename(columns=dict_columns, inplace=True)

    iterations = 10
    resulting_df = pd.DataFrame()

    for col in ['DT']:  # list(data.columns) 'SW', 'CGR', 'NPHI', 'FN', 'PHIE'
        ts = np.array(data[col])
        for i in range(iterations):
            temp_result = run_experiment_with_tuning(ts, col, len_forecast=500, cv_folds=2)
            resulting_df = resulting_df.append(temp_result, ignore_index=True)
        print(resulting_df)
        print(resulting_df.describe())
