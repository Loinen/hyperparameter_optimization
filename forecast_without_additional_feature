import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import timeit
import seaborn as sns
import datetime

from sklearn.metrics import mean_absolute_error, mean_squared_error

from fedot.core.data.data import InputData
from fedot.core.pipelines.node import PrimaryNode, SecondaryNode
from fedot.core.pipelines.pipeline import Pipeline
from fedot.core.pipelines.tuning.unified import PipelineTuner
from fedot.core.repository.dataset_types import DataTypesEnum
from fedot.core.repository.tasks import Task, TaskTypesEnum, TsForecastingParams

# Evolutionary algorithm classes
from fedot.core.optimisers.gp_comp.operators.mutation import MutationTypesEnum
from fedot.core.composer.gp_composer.specific_operators import parameter_change_mutation
from fedot.core.composer.gp_composer.gp_composer import GPComposerBuilder, GPComposerRequirements
from fedot.core.optimisers.gp_comp.gp_optimiser import GPGraphOptimiserParameters
from fedot.core.repository.quality_metrics_repository import MetricsRepository, RegressionMetricsEnum

from main import plot_series, interpolate, comparsion_plot, count_errors


def prepare_input_data(len_forecast, train_data_features, train_data_target, test_data_features):
    task = Task(TaskTypesEnum.ts_forecasting, TsForecastingParams(len_forecast))

    train_input = InputData(idx=np.arange(0, len(train_data_features)),
                            features=train_data_features, target=train_data_target,
                            task=task, data_type=DataTypesEnum.ts)

    start_forecast = len(train_data_features)
    end_forecast = start_forecast + len_forecast
    predict_input = InputData(idx=np.arange(0, end_forecast),
                              features=np.concatenate([train_data_features, test_data_features]),
                              target=None, task=task, data_type=DataTypesEnum.ts)

    return train_input, predict_input, task


def run_experiment_with_tuning(time_series, col_name, len_forecast=250, cv_folds=None):
    # Let's divide our data on train and test samples
    train_data = time_series[:-len_forecast]
    test_data = time_series[-len_forecast:]

    # Source time series
    train_input, predict_input, task = prepare_input_data(len_forecast, train_data_features=train_data,
                                                          train_data_target=train_data,
                                                          test_data_features=train_data)
    init_pipeline = get_complex_pipeline()
    init_pipeline.show()
    print("init pipe")
    init_pipeline.print_structure()

    old_predicted, new_predicted = make_forecast_with_tuning(init_pipeline, train_input, predict_input, task, cv_folds)

    old_predicted = np.ravel(np.array(old_predicted))
    new_predicted = np.ravel(np.array(new_predicted))
    test_data = np.ravel(test_data)

    mse_init, mae_init, mse_tuning, mae_tuning = count_errors(test_data, old_predicted, new_predicted)

    start_point = len(time_series) - len_forecast * 2
    comparsion_plot(col_name, ts, old_predicted, new_predicted, len(train_data), 0,
                    "Init pipeline vs tuned (green) pipeline")
    comparsion_plot(col_name, ts, old_predicted, new_predicted, len(train_data), start_point,
                    "Init pipeline vs tuned (green) pipeline")

    # try to use composer
    # In FEDOT, there are several mutation operators, namely:

    # simple - this type of mutation is passed over all nodes of the tree started from the root node and replace
    # operations in the nodes with new ones;

    # growth - this mutation selects a random node in a tree, generates new subtree,
    # and replaces the selected node's subtree;

    # local_growth - as "growth", but maximal depth of new subtree equals depth of tree located in;

    # reduce - selects a random node in a tree, then removes its subtree. If the current arity of the node's parent
    # is more than the specified minimal arity, then the selected node is also removed;

    # Specific mutation:
    # parameter_change_mutation - this type of mutation is passed over all nodes and changes hyperparameters of the
    # operations;

    mutation_types = [parameter_change_mutation]
    optimiser_parameters = GPGraphOptimiserParameters(mutation_types=mutation_types)
    primary_operations = ['sparse_lagged', 'lagged', 'smoothing', 'scaling',
                          'gaussian_filter', 'ar', 'poly_features', 'normalization', 'pca']
    secondary_operations = ['sparse_lagged', 'lagged', 'linear', 'ridge', 'lasso', 'knnreg', 'dtreg', 'linear',
                            'scaling', 'ransac_lin_reg', 'ransac_non_lin_reg', 'rfe_lin_reg', 'rfe_non_lin_reg']

    composer_requirements = GPComposerRequirements(
        primary=primary_operations,
        secondary=secondary_operations,
        max_arity=4,
        max_depth=8,
        pop_size=10,
        num_of_generations=20, #iterations
        crossover_prob=0.8,
        mutation_prob=0.8,
        timeout=datetime.timedelta(minutes=1), # 5
        cv_folds=2)

    metric_function = MetricsRepository().metric_by_id(RegressionMetricsEnum.MAE)
    builder = GPComposerBuilder(task=task). \
        with_optimiser_parameters(optimiser_parameters). \
        with_requirements(composer_requirements). \
        with_metrics(metric_function).with_initial_pipeline(init_pipeline)
    composer = builder.build()

    obtained_pipeline = composer.compose_pipeline(data=train_input, is_visualise=True)
    obtained_pipeline.fit(train_input)

    # history = obtained_pipeline.history()
    #
    # from fedot.core.visualisation.opt_viz import PipelineEvolutionVisualiser
    #
    # visualiser = PipelineEvolutionVisualiser()
    # visualiser.visualise_history(history)
    # plt.show()

    print("composer pipe")
    obtained_pipeline.print_structure()

    obtained_output = obtained_pipeline.predict(predict_input)
    obtained_forecast = np.ravel(np.array(obtained_output.predict))

    # Мы будем сравнивать ориг пайплайн с новым, затем можно сравнить результаты composer и PipelineTuner
    mse_init, mae_init, mse_composer, mae_composer = count_errors(test_data, old_predicted, obtained_forecast)

    comparsion_plot(col_name, ts, old_predicted, obtained_forecast, len(train_data), start_point,
                    "Init pipeline vs composer (green) pipeline")
    comparsion_plot(col_name, ts, new_predicted, obtained_forecast, len(train_data), start_point,
                    "Tuned pipeline vs composer (green) pipeline")

    now_dt = datetime.datetime.now()
    now = now_dt.strftime("%d_%m_%Y_%H-%M-%S")
    errors_str = ' '.join(str(x) for x in [mse_init, mae_init, mse_tuning, mae_tuning, mse_composer, mae_composer])

    with open('results/errors/errors_{now}.txt'.format(now=now), 'w') as f:
        f.write('mse_init, mae_init, mse_tuning, mae_tuning, mse_composer, mae_composer\n' + errors_str)


def make_forecast_with_tuning(pipeline, train_input, predict_input, task, cv_folds):
    start_time = timeit.default_timer()
    pipeline.fit_from_scratch(train_input)
    amount_of_seconds = timeit.default_timer() - start_time

    print(f'\nIt takes {amount_of_seconds:.2f} seconds to train pipeline\n')

    # Predict orig pipe
    predicted_values = pipeline.predict(predict_input)
    old_predicted_values = predicted_values.predict

    pipeline_tuner = PipelineTuner(pipeline, task, iterations=20, timeout=datetime.timedelta(minutes=1))
    pipeline = pipeline_tuner.tune_pipeline(input_data=train_input, loss_function=mean_squared_error,
                                            loss_params={'squared': False}, cv_folds=cv_folds, validation_blocks=3)

    pipeline.fit_from_scratch(train_input)  # Fit pipeline on the entire train data
    print("PipelineTuner pipe")
    pipeline.print_structure()
    # Predict tuned
    predicted_values = pipeline.predict(predict_input)
    new_predicted_values = predicted_values.predict

    return old_predicted_values, new_predicted_values


def get_complex_pipeline():
    """
    Pipeline looking like this
    smoothing - lagged - ridge \
                                \
                                 ridge -> final forecast
                                /
                lagged - ridge /
    """

    # First level
    node_smoothing = PrimaryNode('smoothing')

    # Second level
    node_lagged_1 = SecondaryNode('lagged', nodes_from=[node_smoothing])
    node_lagged_1.custom_params = {'window_size': 300}  # 300 и 50 - результаты по прошлым запускам
    node_lagged_2 = PrimaryNode('lagged')
    node_lagged_2.custom_params = {'window_size': 50}

    # Third level
    node_ridge_1 = SecondaryNode('ridge', nodes_from=[node_lagged_1])
    node_ridge_2 = SecondaryNode('ridge', nodes_from=[node_lagged_2])

    # Fourth level - root node
    node_final = SecondaryNode('ridge', nodes_from=[node_ridge_1, node_ridge_2])
    pipeline = Pipeline(node_final)

    return pipeline


if __name__ == "__main__":
    data = pd.read_excel("kaggle/well_log.xlsx", sheet_name=0, nrows=3000)
    # убираем лишние столбцы
    data.drop(columns=['SXO', 'Dtsyn', 'Vpsyn', 'sw new', 'sw new%', 'PHI2', 'ΔVp (m/s)', 'ΔVp'], inplace=True)
    print(data)

    plot_series(data)
    plot_series(data[:100])

    corr = data.corr()  # рисуем корреляционную матрицу
    sns.heatmap(corr, annot=True, fmt='.1f', cmap='Blues')
    plt.show()

    data = interpolate(data)
    # our columns - ['Depth', 'CALI', 'CGR', 'DT', 'ILD', 'NPHI', 'PEF', 'PHIE',
    # 'RHOB', 'RT', 'SGR', 'SW', 'Φda', 'ΦN', 'ΦND', 'Vpreal', 'ΔVp']
    # we need to rename some of them
    dict_columns = {'Φda': 'FDA',
                    'ΦN': 'FN',
                    'ΦND': 'FND',
                    'Vpreal': 'VP'}
    data.rename(columns=dict_columns, inplace=True)

    for col in ['DT']:  # list(data.columns) 'SW', 'CGR', 'NPHI', 'FN', 'PHIE'
        ts = np.array(data[col])  # composer and tuning randomly can be different, but not always...
        run_experiment_with_tuning(ts, col, len_forecast=500, cv_folds=2)
